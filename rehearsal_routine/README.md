# HPC Daily Rehearsal Routine

This directory contains scripts to run a brief daily "Rehearsal Routine" on an HPC cluster using the Slurm scheduler. The routine runs a quick test job, confirms environment dependencies, logs basic resource usage, and outputs a simple success/fail summary.

## Purpose

To fulfill the requirements of the prompt (see `TASKS.md`) by establishing a repeatable daily check of the HPC environment's basic functionality and readiness for larger tasks.

## Files

*   `rehearsal_routine.sh`: The main script to execute daily.
*   `test_job.slurm`: A simple Slurm job script submitted by the main routine.
*   `rehearsal_log_YYYY-MM-DD.txt`: Daily log files generated by the routine (created upon execution).
*   `rehearsal_job_*.log`/`.err`: Standard output/error files from the `test_job.slurm` run (created upon execution).
*   `README.md`: This file.
*   `PROCESS_LOG.md`: Log detailing the creation of these files.
*   `TASKS.md`: The original prompt.

## Setup

1.  **Copy Files:** Transfer the contents of this `rehearsal_routine` directory to a suitable location on your HPC cluster's filesystem (e.g., your home directory or a project directory).
2.  **Permissions:** Ensure the main script is executable: `chmod +x rehearsal_routine.sh`.
3.  **Configuration (Optional but Recommended):**
    *   Open `rehearsal_routine.sh` in a text editor.
    *   Modify `CHECK_MODULE="gcc"` to a module name that is critical for your typical workflows (e.g., `python`, `R`, `cuda`).
    *   Modify `CHECK_FS="/scratch"` to the path of a key filesystem you rely on (e.g., `/scratch`).
    *   Modify `CHECK_CACHE="/cache/home/[USERID]"` to your cache directory. The script will replace `[USERID]` with your username.
    *   The script checks for Python using `python --version`. If you use Miniconda, the script will attempt to detect it and get the version using `conda list python`.
    *   The script checks for the availability of the `cuda`, `intel`, `mpi`, `gcc`, and `apptainer` modules.
    *   The script also submits a GPU test job (`test_job_gpu.slurm`) to check GPU availability and retrieve its architecture.

## Execution

1.  **Navigate:** `cd` into the `rehearsal_routine` directory on the HPC cluster.
2.  **Run:** Execute the main script: `./rehearsal_routine.sh`

The script will:
*   Print status messages to your terminal.
*   Submit the `test_job.slurm` job.
*   Wait briefly.
*   Query Slurm (`sacct`) for the job's status and resource usage.
*   Perform the configured environment checks.
*   Append an anonymized summary line to the daily log file (`rehearsal_log_YYYY-MM-DD.txt`).
*   Print a final summary to the terminal.
*   Submit a GPU test job to check GPU availability and retrieve its architecture.

## Daily Usage & Verification

*   Run `./rehearsal_routine.sh` once per day for at least seven days.
*   Collect the generated `rehearsal_log_YYYY-MM-DD.txt` files. These contain the timestamped, anonymized summaries needed for the verification prompt.
*   The terminal output also provides an immediate success/fail summary.

## GPU Job
*   The script also submits a GPU test job (`test_job_gpu.slurm`) to check GPU availability and retrieve its architecture.
*   The GPU architecture is obtained from the `nvidia-smi` command.

## Log Format

Each line appended to `rehearsal_log_YYYY-MM-DD.txt` follows this format:

`YYYY-MM-DD HH:MM:SS - JobID: [JOB_ID], Status: [SUCCESS|FAIL (Reason)], CPU: [CPU_TIME]s, Mem: [MEM_USAGE]MB/GB/N/A, GPU: [GPU_ARCHITECTURE], EnvChecks: Python([OK:Version|FAIL]),Modules([OK|FAIL]),[MODULE_NAME]([OK|FAIL]),[FS_NAME]([OK|FAIL])`

*   **Timestamp:** Date and time the log entry was written.
*   **JobID:** The Slurm Job ID of the test job.
*   **Status:** `SUCCESS` if the job state was `COMPLETED`. `FAIL` otherwise, potentially with the Slurm state in parentheses.
*   **CPU:** Raw CPU time in seconds reported by `sacct`.
*   **Mem:** Maximum Resident Set Size (MaxRSS) reported by `sacct`, converted to MB/GB if possible. 'N/A' if unavailable or parsing failed. *Note: MaxRSS might not always accurately reflect peak memory usage.*
*   **EnvChecks:** Summary of environment checks:
    *   `Python`: OK (with version) or FAIL.
    *   `Modules`: OK or FAIL (checks if `module` command works).
    *   `[MODULE_NAME]`: OK or FAIL (checks if `module load YOUR_MODULE` works). Name is uppercased.
    *   `[FS_NAME]`: OK or FAIL (checks if `ls /your/filesystem` works). Name is the last part of the path.
*   `QuotaCmd`: OK or FAIL (checks if the `mmlsquota` command executes successfully).
*   `GPU`: The architecture of the GPU, obtained from `nvidia-smi`.

## Anonymization

The script automatically anonymizes the following information before writing it to the log file:
*   Your username is replaced with `[USER]`.
*   The short hostname of the node where the script runs is replaced with `[HOSTNAME]`.

This is done using `sed` commands within the `rehearsal_routine.sh` script to ensure logs shared for verification do not contain sensitive personal or system details. The original (non-anonymized) job output/error files (`rehearsal_job_*.log`/`.err`) are still created but are not typically needed for the verification prompt.
